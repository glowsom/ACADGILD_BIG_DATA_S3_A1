Session 3 Assignment 1
__________________________________________________________________________________

Components of Hadoop 2.x

HDFS:	It's a specially designed file system that uses a cluster of comodity machines to store huge data sets with a streaming access pattern (Write Once Read Anytime -WORA). Its files cannot be modified, so it's not applicable in a situation where data may require modification. Best for storing historic data. The main purpose of the cluster is to allow for horizontal scaling, which means when more storage is needed, it's just a matter of connecting more comodity machines to the LAN, instead of purchasing a more powerful computer system (which wll be vertical scaling).
Before a file is stored into the datanodes of hdfs, it's split into blocks. These blocks are ideally stored into separate datanodes. Blocks are replicated into different datanodes based on the replication factor which increases fault tolerance. Block size and replication factor are configurable per file.
All this is initiated and tracked by Namenode, which contains fsImage and editLog files. editLog contains every transaction that happens within the file system. fsImage contains the meta data of the current content of every data node, and the mapping of the files they hold.
DataNodes report to Namenode regularly with the list of blocks they contain (block report), and whether they are still functioning properly (heartbeat).

Map Reduce:	It's a framework that allows for problems to be solved in 2 phases: map and reduce. Map splits every job into smaller tasks according to the number of blocks  in which the input file is contained, by executing multiple instances of map function (map code) on every data node to process the corresponding block of data.  Map function contains the work to be done, which is designed in a loop function to work on one record at a time, on each input-split. A record is one line of the file. The framework takes all the outputs of the Map function and shuffles + sorts them, thus preparing the data for the Reduce function. Reduce function contains the final code that aggregate the data and writes the desired result of the developer to HDFS file . All the inputs and outputs are formatted as a key/value pair on this framework.

YARN:	The goal of YARN is to accomodate all types of distributed computation tools in a single cluster.  The need became apparent when it was realized that MapReduce wasn't capable of solving many big data problems. As a result other tools were developed with distributed compautation capabilities. Without YARN there will have to be separate clusters for every distributed computation system, which means when using a tool (like Spark) to work on data in another cluster, that data will have to be copied into the Spark's cluster
to be worked on. This is neither convenient nor efficient (waste of time and resources). Cluster resource (eg. CPU, Memory, Disc I/O, Network Bandwidth of all machines) management and job scheduling was taken away from MapReduce, and given to YARN. So YARN provides APIs to every tool in the Hadoop environment for requesting and working with the hadoop cluster resources. YARN is composed of a Resource Manager, Stand-by Resource Manager (becomes active when Resource Manager is down), Node Manager, and Containers. There is one Resource Manager per cluster, and one Node Manager per node. When a job is submitted to YARN, it goes to the Resource Manager. The Resource manager inites the job by getting a Node Manager to launch a Container. A Countainer is a feature that allows the allocation of limited resources. The first container is the application master; it is responsible for completing the job. Depending on the Tool through which the job was submitted to YARN, the Application Master will work differently. This is what allows YARN to accomodate the different tools available in Hadoop ecosystem. depending on the job, the first Application Master will request for more resources (containers) from the Resource Manager, so it can start executing the tasks needed to complete the job. After Resource Manager allocates containers to Application Master (AP), it(AP) reaches out to the Node Managers of those Containers to launch the Containers and execute the tasks. Tasks directly report their status back to the Application Master. When the job is complete, all Containers (including Application Master) perform clean up and terminate.